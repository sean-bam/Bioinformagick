{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string \n",
    "import random\n",
    "\n",
    "import drivers\n",
    "import parsers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from Bio import AlignIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "import Bio.PDB.Dice as BPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-lending",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spanish-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neff(alignment):\n",
    "    \"\"\"\n",
    "    Calculate the Neff of an MSA\n",
    "    Neff is defined as the number of clusters at 90% ID over / 33% length\n",
    "    \"\"\"\n",
    "    subprocess.run(f'fa_strict -l=0 {alignment} > tmp.afa',\n",
    "                  shell = True,\n",
    "                  check = True)\n",
    "    \n",
    "    p1 = subprocess.run(f'run_mmclust tmp.afa -w=0 -s=0.9 -c=.33 -lin | cut -f2 | sort | uniq | wc -l',\n",
    "               shell = True,\n",
    "                text = True,\n",
    "                check = True,\n",
    "               capture_output = True\n",
    "                )\n",
    "    \n",
    "    neff = p1.stdout\n",
    "    \n",
    "    Path('tmp.afa').unlink()\n",
    "    \n",
    "    return neff\n",
    "    \n",
    "def colabfold_logfile_to_table(logfile, output):\n",
    "    \n",
    "    with open(output, 'w') as o:\n",
    "        with open(logfile) as f:\n",
    "            for line in f:\n",
    "                if 'Query' in line:\n",
    "                    jobname = line.strip().split()[4]\n",
    "\n",
    "                if 'took' in line:\n",
    "                    model = line.strip().split()[2]\n",
    "\n",
    "                    time_raw = line.strip().split()[4]\n",
    "                    time = time_raw.split(\".\")[0]\n",
    "\n",
    "                    pLDDT = line.strip().split()[7]\n",
    "                    print(jobname,model,time,pLDDT, sep = \",\", file = o)\n",
    "                    \n",
    "def colabfold_logtable_to_df(table):\n",
    "    \n",
    "    df = pd.read_csv(table, \n",
    "                    header = None, \n",
    "                    names = ['profile', \n",
    "                             'model', \n",
    "                             'time', \n",
    "                             'score']\n",
    "                    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ranked_models(af_output_dir):\n",
    "    \"\"\"\n",
    "    Searches through a directory containing *.pdb files produced from alphafold/colabfold\n",
    "    Selects the top ranked file based on the presence of \"rank_1\" in the filename\n",
    "    Returns a list\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in Path(af_output_dir).rglob('*.pdb'):\n",
    "        if \"rank_1\" in str(file):\n",
    "            files.append(file)\n",
    "    return files\n",
    "\n",
    "def pdb_id_generator(size=4, chars=string.ascii_uppercase + string.digits):\n",
    "    \"\"\"\n",
    "    Generates a random 4 character string\n",
    "    from: \n",
    "    https://stackoverflow.com/questions/2257441/random-string-generation-with-upper-case-letters-and-digits\n",
    "    \"\"\"\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "def dali_import_private_structure(filelist, output):\n",
    "    \"\"\"\n",
    "    Expects a list of file path objects\n",
    "    Runs import.pl, assigns a random PDB ID to each file\n",
    "    \"\"\"\n",
    "    output_dir = Path(output)\n",
    "    file_to_pdb_lookup_table = output_dir / Path('pdbids.csv')\n",
    "    \n",
    "    with open(file_to_pdb_lookup_table, 'w') as o:\n",
    "        for file in filelist:\n",
    "            \n",
    "            file_base = re.sub(\"_unrelaxed_model_._rank_1\", \"\", file.stem)\n",
    "\n",
    "            pdbid = pdb_id_generator()\n",
    "\n",
    "            subprocess.run(f'import.pl --pdbfile {file} --pdbid {pdbid} --dat {output}',\n",
    "                           shell = True,\n",
    "                           check = True\n",
    "                          )\n",
    "\n",
    "            print(file_base,\n",
    "                  file.stem, \n",
    "                  pdbid, \n",
    "                  sep = \",\",\n",
    "                  file = o)\n",
    "            \n",
    "def parse_dali(dalioutput):\n",
    "    \"\"\"\n",
    "    Gets the top 9 hits from a dali 'summary' output\n",
    "    Clearly, needs improvement..\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(dalioutput) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"# Query:\"):\n",
    "                query = line.strip().split()[2]\n",
    "            if line.startswith(\"   \"):\n",
    "                hit_no, chain, z, rmsd, lali, nres, identity, description = line.strip().split(maxsplit=7)\n",
    "                data.append({\"query_id\" : query,\n",
    "                             \"chain\" : chain,\n",
    "                             \"z\" : z,\n",
    "                             \"rmsd\" : rmsd,\n",
    "                             \"lali\" : lali,\n",
    "                             \"nres\" : nres,\n",
    "                             \"identity\" : identity,\n",
    "                             \"description\" : description})\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metric-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_pdb(pdbid, pdbfile, start, end, output):\n",
    "    \"\"\"\n",
    "    Accepts a PDBfile and start/end coordinates\n",
    "    Outputs a new file with just the structure between start-end\n",
    "    \n",
    "    End can be larger than length of the molecule\n",
    "    \"\"\"\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure(pdbid, pdbfile)\n",
    "    BPD.extract(structure, 'A', start, end, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-timothy",
   "metadata": {},
   "source": [
    "I tried adding this code on line 1016 in `batch.py`, but it didn't work\n",
    "```\n",
    "summary_file = result_dir / Path('summary.txt')\n",
    "if summary_file.exists():\n",
    "    summary_file.unlink()\n",
    "\n",
    "with summary_file.open('a') as f:\n",
    "    for model, score_dict in outs.items():\n",
    "        for metric, scores in score_dict.items():\n",
    "            print(jobname,model,metric,scores, sep = \",\", file = f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-intervention",
   "metadata": {},
   "source": [
    "# MSAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-editing",
   "metadata": {},
   "source": [
    "## CAS I-B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-modern",
   "metadata": {},
   "source": [
    "Kira sent me a table of MSAs corresponding to CAS I-B (`/profiles/casIB/casIBmetadata.csv`). I obtained the sequences from her directory:\n",
    ">for id in `cut -f1 casIBmetadata.csv -d\",\"`; do cp /net/frosty/vol/export1/proteome/cascl19/projFTP/profiles/${id}.FASTA .; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-target",
   "metadata": {},
   "source": [
    "Calculate the Neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('profiles/casIB/neff.csv', 'w') as o:\n",
    "    for file in Path('profiles/casIB/').rglob('*.FASTA'):\n",
    "        neff = get_neff(file)\n",
    "        print(file.stem, neff, sep = \",\", file = o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "Make A3Ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in Path('profiles/casIB/').rglob('*.FASTA'):\n",
    "    a3m = file.with_suffix('.a3m')\n",
    "    !reformat.pl fas a3m -M first {file} {a3m}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-porcelain",
   "metadata": {},
   "source": [
    "## Natalya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-parallel",
   "metadata": {},
   "source": [
    "Natalya sent me 75 MSAs of unknown origin. Each MSA has the consensus (*.afac) or not (*.afa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('profiles/natalya/neff.csv', 'w') as o:\n",
    "    for file in Path('profiles/natalya/').rglob('*.afa'):\n",
    "        neff = get_neff(file)\n",
    "        print(file.stem, neff, sep = \",\", file = o)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "Make A3Ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in Path('profiles/natalya/').rglob('*.afac'):\n",
    "    a3m = file.with_suffix('.a3m')\n",
    "    !reformat.pl fas a3m -M first {file} {a3m}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-model",
   "metadata": {},
   "source": [
    "## crAssphage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-chain",
   "metadata": {},
   "source": [
    "I downloaded `278` the profiles from crassphages from natalyas FTP site. All of them have consensus:\n",
    ">wget -m -e robots=off --no-parent  ftp.ncbi.nih.gov/pub/yutinn/crassfamily_2020/profiles/\n",
    "\n",
    "The longest is `VP02650` at 1608 positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-spider",
   "metadata": {},
   "source": [
    "Calculate the Neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('profiles/crass/neff.csv', 'w') as o:\n",
    "    for file in Path('profiles/crass/').rglob('*.afac'):\n",
    "        neff = get_neff(file)\n",
    "        print(file.stem, neff, sep = \",\", file = o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "Make A3Ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in Path('profiles/crass/').rglob('*.afac'):\n",
    "    a3m = file.with_suffix('.a3m')\n",
    "    !reformat.pl fas a3m -M first {file} {a3m}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-newfoundland",
   "metadata": {},
   "source": [
    "## Hypervariable ORFs\n",
    "\n",
    "In my gutphage paper, I found DGRs targeting genes. Get ones that have unknown function, and filter the MSA b/c I didn't use a good coverage criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dgr = pd.read_csv('/home/benlersm/projects/human_virome_project/DGR/DGRs_hhpred.csv').rename(columns = {'query' : 'prot_id'})\n",
    "prot_ids = df_dgr.query('prob < 60').prot_id.tolist()\n",
    "for prot_id in prot_ids:\n",
    "    a3m = Path(f'/home/benlersm/projects/human_virome_project/DGR/rt_contigs/VR_orfs/{prot_id}.a3m')\n",
    "    #!cp /home/benlersm/projects/human_virome_project/DGR/rt_contigs/VR_orfs/{prot_id}.a3m profiles/dgr/\n",
    "    !hhfilter -i {a3m} -o profiles/dgr/{prot_id}.a3m -id 100 -cov 90 -qid 33\n",
    "    !reformat.pl a3m fas profiles/dgr/{prot_id}.a3m profiles/dgr/{prot_id}.afa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-concern",
   "metadata": {},
   "source": [
    "delete the smaller alignment files\n",
    "```\n",
    "find . -name \"*.a3m\" -size -10k  -delete\n",
    "find . -name \"*.afa\" -size -10k  -delete\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-reporter",
   "metadata": {},
   "source": [
    "manually restore 2 a3m files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('profiles/dgr/neff.csv', 'w') as o:\n",
    "    for file in Path('profiles/dgr/').rglob('*.afa'):\n",
    "        neff = get_neff(file)\n",
    "        print(file.stem, neff, sep = \",\", file = o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-assignment",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "collective-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in Path('profiles/').rglob('neff.csv'):\n",
    "    df = pd.read_csv(file, header = None, names = [\"profile\", \"neff\"])\n",
    "    df_list.append(df)\n",
    "df_neff = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-scottish",
   "metadata": {},
   "source": [
    "Get the length of each MSA using Biopython http://biopython.org/DIST/docs/tutorial/Tutorial.html#:~:text=You%E2%80%99ll-,notice,-in%20the%20above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "enormous-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in Path('profiles/casIB/').rglob('*.FASTA'):\n",
    "    alignment = AlignIO.read(file, \"fasta\").get_alignment_length()\n",
    "    data.append({\"profile\" : file.stem,\n",
    "                 \"length\" : alignment})\n",
    "    #print(file.stem,alignment) \n",
    "\n",
    "for file in Path('profiles/').rglob('*.afac'):\n",
    "    alignment = AlignIO.read(file, \"fasta\").get_alignment_length()\n",
    "    data.append({\"profile\" : file.stem,\n",
    "                 \"length\" : alignment})\n",
    "    \n",
    "for file in Path('profiles/').rglob('*.afa'):\n",
    "    alignment = AlignIO.read(file, \"fasta\").get_alignment_length()\n",
    "    data.append({\"profile\" : file.stem,\n",
    "                 \"length\" : alignment})\n",
    "    \n",
    "\n",
    "df_len = pd.DataFrame.from_records(data).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "quarterly-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.merge(df_neff, df_len, how = 'left', on = 'profile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-theta",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-burning",
   "metadata": {},
   "source": [
    "## Select MSAs for analysis\n",
    "\n",
    "Get deep MSAs (Neff > 20), and ~40 short and shallow MSAs (len < 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deep = df_metadata.query('neff > 20')\n",
    "profile_list_neff20 = df_deep.profile.tolist()\n",
    "\n",
    "df_shallow = df_metadata.query('neff <= 20 and length < 150')\n",
    "profile_list_shallow = df_shallow.profile.tolist()\n",
    "profile_list = profile_list_neff20 + profile_list_shallow\n",
    "len(profile_list)\n",
    "#profile_list_neff20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profile_list:\n",
    "\n",
    "    msa = list(Path('profiles/').rglob(f'{profile}.a3m'))[0]\n",
    "    if not Path(msa).exists():\n",
    "        print(msa)\n",
    "    \n",
    "    !cp {msa} input/exp1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-fellowship",
   "metadata": {},
   "source": [
    "## Exp 1: Number of models versus PAE\n",
    "\n",
    "Test if running with 1 or 5 models does anything (5 is the max)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-ordering",
   "metadata": {},
   "source": [
    "Make an input folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-shelf",
   "metadata": {},
   "source": [
    "run colabfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-staff",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 colabfold_batch --num-models 1 input/exp1/ output/exp1/models1/ &> /dev/null\n",
    "CUDA_VISIBLE_DEVICES=1 colabfold_batch --num-models 5 input/exp1/ output/exp1/models5/\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-theory",
   "metadata": {},
   "source": [
    "## Exp 2 : Number of recycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-isolation",
   "metadata": {},
   "source": [
    "The results from exp1 show that adding 5 models increases confidence by ~0.3 on average, so I will just use one model from here on out to speed things up. In my dataset of 63 MSAs with Neff > 20, it took ~3 hours to fold these with all other parameters set as default.\n",
    "\n",
    "Test explicitly setting the number of recycles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-virus",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 colabfold_batch --num-models 1 --num-recycle 10 input/exp1/ output/exp2/recyle10/\n",
    "CUDA_VISIBLE_DEVICES=1 colabfold_batch --num-models 1 --num-recycle 5 input/exp1/ output/exp2/recyle5/\n",
    "CUDA_VISIBLE_DEVICES=2 colabfold_batch --num-models 1 --num-recycle 3 input/exp1/ output/exp2/recyle3/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-convertible",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-confirmation",
   "metadata": {},
   "source": [
    "## Exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "pursuant-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>score_model1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>VP02569</td>\n",
       "      <td>model_3</td>\n",
       "      <td>0</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    profile    model  time  score_model1\n",
       "80  VP02569  model_3     0          66.4"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colabfold_logfile_to_table('output/exp1/models1/log.txt', 'output/exp1/models1/log.csv')\n",
    "df_model1 = (colabfold_logtable_to_df('output/exp1/models1/log.csv')\n",
    "            .rename(columns = {\"score\" : \"score_model1\"}\n",
    "                   )\n",
    "            )\n",
    "\n",
    "            \n",
    "#df_model1[\"num_models\"] = 1\n",
    "df_model1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "loving-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>score_model5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>VP02663</td>\n",
       "      <td>81.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     profile  score_model5\n",
       "430  VP02663          81.8"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#models5 is accidently in models10 folder\n",
    "colabfold_logfile_to_table('output/exp1/models10/log.txt', 'output/exp1/models10/log.csv')\n",
    "df_model5 = (colabfold_logtable_to_df('output/exp1/models10/log.csv')\n",
    "            .sort_values('score', ascending = False)\n",
    "             .drop_duplicates('profile')\n",
    "             .drop(columns = ['model', 'time'])\n",
    "            .rename(columns = {\"score\" : \"score_model5\"}\n",
    "            \n",
    "                   )\n",
    "            )\n",
    "df_model5.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "residential-industry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>score_model1</th>\n",
       "      <th>score_model5</th>\n",
       "      <th>delta_5_models_vs_1_model</th>\n",
       "      <th>neff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>VP00411</td>\n",
       "      <td>model_3</td>\n",
       "      <td>0</td>\n",
       "      <td>86.1</td>\n",
       "      <td>86.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    profile    model  time  score_model1  score_model5  \\\n",
       "40  VP00411  model_3     0          86.1          86.1   \n",
       "\n",
       "    delta_5_models_vs_1_model  neff  \n",
       "40                        0.0    24  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df_model1, df_model5, how = 'left', on = 'profile')\n",
    "df[\"delta_5_models_vs_1_model\"] = df[\"score_model5\"] - df[\"score_model1\"]\n",
    "\n",
    "df2 = pd.merge(df, df_neff, how = 'left', on = 'profile')\n",
    "df2.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df2, x = 'neff', y = 'score_model5', template = 'simple_white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.scatter(df2, x= 'neff', y= 'score_model5', color = 'delta_5_models_vs_1_model')\n",
    "px.box(df2, y = 'delta_5_models_vs_1_model', template = 'simple_white', width = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "recognized-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.sort_values('delta_5_models_vs_1_model', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-reality",
   "metadata": {},
   "source": [
    "## Exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "intellectual-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "colabfold_logfile_to_table('output/exp2/recyle10/log.txt', 'output/exp2/recyle10/log.csv')\n",
    "df_recycle10 = (colabfold_logtable_to_df('output/exp2/recyle10/log.csv'))         \n",
    "df_recycle10[\"num_recycles\"] = 10\n",
    "\n",
    "\n",
    "colabfold_logfile_to_table('output/exp2/recycle5/log.txt', 'output/exp2/recycle5/log.csv')\n",
    "df_recycle5 = (colabfold_logtable_to_df('output/exp2/recycle5/log.csv'))\n",
    "df_recycle5[\"num_recycles\"] = 5\n",
    "\n",
    "colabfold_logfile_to_table('output/exp2/recycle3/log.txt', 'output/exp2/recycle3/log.csv')\n",
    "df_recycle3 = (colabfold_logtable_to_df('output/exp2/recycle3/log.csv'))\n",
    "df_recycle3[\"num_recycles\"] = 3\n",
    "\n",
    "df_recycle3.sample()\n",
    "\n",
    "df_recycle = pd.concat([df_recycle3, df_recycle5, df_recycle10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(df_recycle, x = 'num_recycles', y = 'score', template = 'simple_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-cincinnati",
   "metadata": {},
   "source": [
    "# DALI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-printing",
   "metadata": {},
   "source": [
    "## Install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-lloyd",
   "metadata": {},
   "source": [
    "The current version of DALI was not compiled using `make parallel`, because there is no `mpicompare` in the `bin/` directory of DALI:\n",
    "\n",
    "```\n",
    "mpirun was unable to launch the specified application as it could not access\n",
    "or execute an executable:\n",
    "\n",
    "Executable: /usr/local/DALI/5.0/bin/mpicompare\n",
    "Node: sge897\n",
    "\n",
    "while attempting to start process rank 0.\n",
    "```\n",
    "\n",
    "So, I reinstalled my own copy for now in the `hhsuite_db` folder.\n",
    "\n",
    "However, I get a different openmpi error when trying to run in parallel:\n",
    "\n",
    "```\n",
    "# /usr/local/openmpi/1.4.1/bin/mpirun --np 10 /panfs/pan1.be-md.ncbi.nlm.nih.gov/hhsuite_db/DaliLite.v5/bin/mpicompare output/exp1/models10/dat/ output/exp1/models10/dat/ DALICON T < dalicon_input > /dev/null\n",
    "[sge619:12640] [[8402,0],0] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge766:05511] [[8402,0],1] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge586:18066] [[8402,0],2] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge992:07001] [[8402,0],4] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge322:03494] [[8402,0],5] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge354:19330] [[8402,0],3] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge639:07790] [[8402,0],6] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge1049:06962] [[8402,0],7] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge1040:13885] [[8402,0],9] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "[sge672:22537] [[8402,0],8] mca_oob_tcp_recv_handler: invalid message type: 15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-pixel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "protected-exclusive",
   "metadata": {},
   "source": [
    "## database Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-edwards",
   "metadata": {},
   "source": [
    "DALI requires PDB files be converted into an internal format. There are a few ways to do this. Ultimately, I chose option 3, which using both option 1 and option 2 outputs. \n",
    "\n",
    "Basically, make a mirror of the PDB database using the `--rsync` option of `import.pl` (option 1). This will try to import every file in the PDB archive, but will crash. Then, get a list of cluster representatives from the PDB website (option 2). Use this list to run `import.pl` for the PDB archive.\n",
    "\n",
    "In the future, try just getting a copy of the PDB archive without running `import.pl --rsync`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-development",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-apartment",
   "metadata": {},
   "source": [
    "The first option is to run the `impoort.pl` script with `--rsync`, which downloads the entire PDB. For each PDB entry, it gets a `*.ent.gz`* file and then converts every single entry to the internal data format. I'll try this option first:\n",
    ">import.pl --rsync --pdbmirrordir dbs/pdb/ --dat dbs/dali/dali_pdb/DAT/ --clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-passage",
   "metadata": {},
   "source": [
    "However, I get this error:\n",
    "```\n",
    "Reading DSSP file\n",
    "At line 288 of file ../src/util.f\n",
    "Fortran runtime error: Bad integer for item 1 in list input\n",
    "Error in puu: /usr/local/DALI/5.0/bin/puu 5a1vU 32240.tmp 5a1v.dssp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-saint",
   "metadata": {},
   "source": [
    "There is something wrong with one of the PDB chains that is breaking the automatic import. Once `import.pl` breaks, it creates a `dali.lock` file that prevents subsequent imports. The second problem is that it appears that `250000` files is the max that can be put into a folder on our system, and there are ~450000 PDB entries. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-syntax",
   "metadata": {},
   "source": [
    "### option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-andrews",
   "metadata": {},
   "source": [
    "Rather than doing the whole PDB via `import.pl --rsync`, just do a subset. \n",
    "\n",
    "PDB regularly makes a subset clustered with MMSeqs2 at various thresholds, available [here](https://www.rcsb.org/docs/programmatic-access/file-download-services#:~:text=4HHB.A/download-,Sequence%20clusters%20data,-Results%20of%20the). I can download the PDB files using the PDB script `batch_download.sh`, available [here](https://www.rcsb.org/docs/programmatic-access/batch-downloads-with-shell-script). The problem with this approach is that **some pdb entries are too large to download the .pdb.gz file and would take ~1 day to complete**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are ~57k lines in bc-70.out\n",
    "#each line is a cluster. I am assuming the first entry is the representative\n",
    "#batch_download requires a file with a comma-separated list of IDs:\n",
    "\n",
    "pdb_ids = []\n",
    "with open('dbs/pdb70/bc-70.out') as f:\n",
    "    for line in f:\n",
    "\n",
    "        #get the first listed entry\n",
    "        rep = line.strip().split()[0]\n",
    "\n",
    "        #get the base PDB id\n",
    "        pdb_id = re.sub(\"_.\", \"\", rep)\n",
    "\n",
    "        pdb_ids.append(pdb_id)\n",
    "with open('dbs/pdb70/bc-70.csv', 'w') as o:\n",
    "    print(\",\".join(pdb_ids), file = o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-discipline",
   "metadata": {},
   "source": [
    "Download the PDB files. This will take ~1day\n",
    ">./batch_download.sh -f pdb70/bc-70.csv -o pdb70/ -p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-weight",
   "metadata": {},
   "source": [
    "Import the files **TBD**. Again, some files crash `import.pl` and create a `dali.lock` file, so i have to import them individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-piano",
   "metadata": {},
   "source": [
    "### Option 3\n",
    "\n",
    "Option 2 is too slow. PDB provides an rsync script that mirrors the archive, available [here](https://www.rcsb.org/docs/programmatic-access/file-download-services#:~:text=the%20ftp%20protocol.-,Automated%20download%20of%20data,-The%20URLs%20in). My guess is this is what `import.pl --rysnc` uses.\n",
    "\n",
    "So, use the `*.ent` files generated in option 1, and the list of representatives in `option 2`, to import the files.\n",
    "\n",
    "Again, If I use `import.pl --pdblist pdb70_subset.list`, where `pdb70_subset.list` is a list of file paths to `*.ent.gz` files, I run into the same issue where a bad file crashes the program. So, I import individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "lightweight-joyce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57256"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmreps = []\n",
    "\n",
    "with open('dbs/dali/dali_pdb/bc-70.out') as f:\n",
    "    for line in f:\n",
    "\n",
    "        #get the first listed entry\n",
    "        rep = line.strip().split()[0]\n",
    "\n",
    "        #get the base PDB id. Double check everything is uppercase\n",
    "        pdb_id = rep[0:4].upper()\n",
    "\n",
    "        mmreps.append(pdb_id)\n",
    "len(mmreps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-extension",
   "metadata": {},
   "source": [
    "Loop through the PDB directory containing all PDB files. IF the file is a representative, run `import.pl` to extract all the chains. \n",
    "\n",
    "A better way would be to loop through the representatives, find the file using the middle two characters of the PDB ID, and run import.pl. However, I already did it this way, so whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for file in Path('dbs/pdb/').rglob('*.ent.gz'):\n",
    "\n",
    "    pdb_id = file.stem.strip('pdb').strip('.ent')\n",
    "\n",
    "    if pdb_id.upper() in mmreps:\n",
    "        i += 1\n",
    "        \n",
    "        #check if the file has been imported already\n",
    "        datfile_gen = Path(f'dbs/dali/dali_pdb/pdb70/').glob(f'{pdb_id}*.dat')\n",
    "        if len(list(datfile_gen)) < 1:\n",
    "        \n",
    "            print(f\"importing {pdb_id}\")\n",
    "            subprocess.run(f'import.pl --pdbfile {file} --pdbid {pdb_id} --dat dbs/dali/dali_pdb/pdb70/ --clean',\n",
    "                         shell = True,\n",
    "                         #check = True\n",
    "                          )\n",
    "\n",
    "            if Path('dali.lock').exists():\n",
    "                print(f\"{pdb_id} crashed import.pl, skipping\")\n",
    "                Path('dali.lock').unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-albania",
   "metadata": {},
   "source": [
    "Import.pl extracts all chains from a PDB file, but only some of them are actually representatives. Remove the non-representative DAT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "isolated-rabbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 57256 representatives\n",
      "Of those, there 47893 that have a matching .DAT file\n",
      "will delete 94855 files\n"
     ]
    }
   ],
   "source": [
    "mmreps = []\n",
    "with open('dbs/dali/dali_pdb/bc-70.out') as f:\n",
    "    for line in f:\n",
    "\n",
    "        #get the first listed entry\n",
    "        rep = line.strip().split()[0]\n",
    "\n",
    "        #Double check everything is uppercase\n",
    "        mmreps.append(rep.upper())\n",
    "\n",
    "i=0\n",
    "files_to_delete = []\n",
    "for file in Path('dbs/dali/dali_pdb/pdb70/').rglob('*.dat'):\n",
    "    pdb_id = file.stem[0:4]\n",
    "    chain = file.stem[4:]\n",
    "    pdb_id_with_chain = (pdb_id + \"_\" + chain).upper()\n",
    "    \n",
    "    if not pdb_id_with_chain in mmreps:\n",
    "        files_to_delete.append(file)\n",
    "    else:\n",
    "        i+=1\n",
    "print(f'there are {len(mmreps)} representatives')\n",
    "print(f'Of those, there {i} that have a matching .DAT file')\n",
    "print(f'will delete {len(files_to_delete)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "unlike-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_to_delete:\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-default",
   "metadata": {},
   "source": [
    "Note the discrepancy between the number of MMcluste representatives and the number of matching DAT files. \n",
    "\n",
    "I checked a couple of PDB IDs. It appears that `import.pl` doesn't produce a DATfile for some of them (E.g., 1a11). On the dali website, chains shorter than 30 AA are excluded, so it might be because of this. \n",
    "\n",
    "Also, if I using the PDB `batch_download.sh` script to download PDB files individually, I end up with `46828` files, not `~57000`. \n",
    "\n",
    "So, I don't think there is something seriously wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-syndication",
   "metadata": {},
   "source": [
    "Make a tarball of the directory\n",
    ">tar -czvf pdb70datfiles.tar.gz pdb70/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-glory",
   "metadata": {},
   "source": [
    "Make a list of the DAT files to constrain the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "racial-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dbs/dali/dali_pdb/pdb70.list', 'w') as o:\n",
    "    for file in Path('dbs/dali/dali_pdb/pdb70/').rglob('*.dat'):\n",
    "        print(file.stem, file = o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-friday",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-creek",
   "metadata": {},
   "source": [
    "### Using openmpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-muscle",
   "metadata": {},
   "source": [
    "```\n",
    "#make a swarmfile\n",
    "echo DaliLite.v5/bin/dali.pl --cd1 T43NA --dat1 output/exp1/models10/dat/ --dat2 dbs/dali/dali_pdb/pdb70/ --clean --oneway --hierarchical --repset small.list --db small.list --np 10 --MPIRUN_EXE /usr/local/openmpi/1.4.1/bin/mpirun > test.swarm\n",
    "\n",
    "#make a SGEfarm submit .sh script\n",
    "\n",
    "\n",
    "#edit the .sh file, and add the following line\n",
    "#$ -pe openmpi 10\n",
    "\n",
    "#submit\n",
    "qsub job.submitSGEfarm.68050.1.csh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-warning",
   "metadata": {},
   "source": [
    "### Using the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-failing",
   "metadata": {},
   "source": [
    "An example command:\n",
    ">dali.pl --cd1 T43NA --dat1 output/exp1/models10/dat/ --dat2 dbs/dali/dali_pdb/pdb70/ --clean --oneway --hierarchical --repset pdb70.list --db pdb70.list --outfmt \"summary,alignments\"\n",
    "\n",
    "- `cd1` : Name of the (fake) PDB ID of the .DAT file\n",
    "- `dat1` : location of the .DAT file. If using a PDB file as a query instead, this is where a temporary .DAT file is made (defaults to `./DAT`, throws an error if this dir isn't present\n",
    "- `dat2` : location of the database .DAT files\n",
    "- `repset` : only consider a subset of files\n",
    "- `db` : I'm not sure if this is needed or not when using `repset`.\n",
    "\n",
    "Other params\n",
    "\n",
    "-`pdbid1` : name of the output file (defaults to mol1A)\n",
    "\n",
    "The program makes a lot of temporary files, so I am not sure I can parallelize using a swarmfile. And, given the current problems with `openmpi`, i can only use 1 thread. In a test run, it took ~50 mins for 1 query against hte pdb70 subset.\n",
    "\n",
    "I think one workaround is to use BioWulf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-enhancement",
   "metadata": {},
   "source": [
    "### Using BioWulf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-aircraft",
   "metadata": {},
   "source": [
    "Make a mini sbatch command on CBBDev machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-personality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decreased-julian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>VP00399</td>\n",
       "      <td>model_5</td>\n",
       "      <td>0</td>\n",
       "      <td>77.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    profile    model  time  score\n",
       "37  VP00399  model_5     0   77.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the alphafold scores\n",
    "colabfold_logfile_to_table('output/exp1/models10/log.txt', 'output/exp1/models10/log.csv')\n",
    "df_model10 = colabfold_logtable_to_df('output/exp1/models10/log.csv')\n",
    "            \n",
    "df_model10.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "included-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make DAT files for the top structures\n",
    "top_models = get_top_ranked_models('output/exp1/models10/')\n",
    "dali_import_private_structure(top_models, 'output/exp1/models10/dat/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "presidential-marketing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>modelfile</th>\n",
       "      <th>pdbid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>VP02626</td>\n",
       "      <td>VP02626_unrelaxed_model_4_rank_1</td>\n",
       "      <td>BNUW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    profile                         modelfile pdbid\n",
       "65  VP02626  VP02626_unrelaxed_model_4_rank_1  BNUW"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dali = pd.read_csv('output/exp1/models10/dat/pdbids.csv', \n",
    "                      header = None, \n",
    "                      names = [\"profile\",\n",
    "                               \"modelfile\",\n",
    "                               \"pdbid\"\n",
    "                              ]\n",
    "                     )\n",
    "df_dali.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amino-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>modelfile</th>\n",
       "      <th>pdbid</th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VP02761</td>\n",
       "      <td>VP02761_unrelaxed_model_3_rank_1</td>\n",
       "      <td>X7WL</td>\n",
       "      <td>model_3</td>\n",
       "      <td>0</td>\n",
       "      <td>94.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VP02761</td>\n",
       "      <td>VP02761_unrelaxed_model_3_rank_1</td>\n",
       "      <td>X7WL</td>\n",
       "      <td>model_4</td>\n",
       "      <td>0</td>\n",
       "      <td>94.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VP02761</td>\n",
       "      <td>VP02761_unrelaxed_model_3_rank_1</td>\n",
       "      <td>X7WL</td>\n",
       "      <td>model_5</td>\n",
       "      <td>0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VP02761</td>\n",
       "      <td>VP02761_unrelaxed_model_3_rank_1</td>\n",
       "      <td>X7WL</td>\n",
       "      <td>model_1</td>\n",
       "      <td>0</td>\n",
       "      <td>92.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VP02761</td>\n",
       "      <td>VP02761_unrelaxed_model_3_rank_1</td>\n",
       "      <td>X7WL</td>\n",
       "      <td>model_2</td>\n",
       "      <td>0</td>\n",
       "      <td>92.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VP02726</td>\n",
       "      <td>VP02726_unrelaxed_model_3_rank_1</td>\n",
       "      <td>A9HD</td>\n",
       "      <td>model_3</td>\n",
       "      <td>0</td>\n",
       "      <td>91.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   profile                         modelfile pdbid    model  time  score\n",
       "0  VP02761  VP02761_unrelaxed_model_3_rank_1  X7WL  model_3     0   94.5\n",
       "1  VP02761  VP02761_unrelaxed_model_3_rank_1  X7WL  model_4     0   94.3\n",
       "2  VP02761  VP02761_unrelaxed_model_3_rank_1  X7WL  model_5     0   94.0\n",
       "3  VP02761  VP02761_unrelaxed_model_3_rank_1  X7WL  model_1     0   92.4\n",
       "4  VP02761  VP02761_unrelaxed_model_3_rank_1  X7WL  model_2     0   92.9\n",
       "5  VP02726  VP02726_unrelaxed_model_3_rank_1  A9HD  model_3     0   91.3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dali2 = pd.merge(df_dali,\n",
    "                    df_model10,\n",
    "                    how = 'left',\n",
    "                    on = 'profile')\n",
    "df_dali2.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elementary-bibliography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dali3 = (df_dali2.sort_values('score', ascending = False)\n",
    "                    .drop_duplicates('profile')\n",
    "                    .query('score > 70')\n",
    "           )\n",
    "df_dali3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-transparency",
   "metadata": {},
   "source": [
    "Make a batch file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ruled-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dali.sh', 'w') as o:\n",
    "    for pdbid in df_dali3.pdbid.tolist():\n",
    "            print(f'dali.pl --cd1 {pdbid}A --dat1 output/exp1/models10/dat/ --dat2 dbs/dali/dali_pdb/pdb70/ --clean --oneway --hierarchical --repset pdb70.list --db pdb70.list --outfmt \"summary,alignments\" --np $SLURM_NTASKS --MPIRUN_EXE /usr/local/OpenMPI/4.1.1/gcc-9.2.0/bin/mpirun',\n",
    "                  file = o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-rally",
   "metadata": {},
   "source": [
    "sbatch --partition=multinode --constraint=x2650 --ntasks=64 --ntasks-per-core=1 --time=08:00:00 --qos=turbo --exclusive dali.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-collaboration",
   "metadata": {},
   "source": [
    "It took 20 mins to run one protein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-mystery",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "short-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in Path('output_dali/').rglob('*.txt'):\n",
    "    df = parse_dali(file)\n",
    "    df_list.append(df)\n",
    "df2 = pd.concat(df_list).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beautiful-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.sort_values('z', ascending = False)#.drop_duplicates('query_id')\n",
    "\n",
    "#add A to the PDB ID chain so it matches DALI output\n",
    "df_dali3[\"pdbid2\"] = df_dali3[\"pdbid\"].astype('str') + str(\"A\")\n",
    "\n",
    "df4 = pd.merge(df3,\n",
    "               df_dali3, \n",
    "               how = 'left', \n",
    "               left_on = 'query_id', \n",
    "               right_on = 'pdbid2')\n",
    "#df4.to_csv('dali_results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "meaningful-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df4.query('profile.str.contains(\"cas\") or profile.str.contains(\"Cas\")')\n",
    "     .sort_values([\"profile\", \n",
    "                   \"z\"], \n",
    "                  ascending = [False,False])\n",
    "     .loc[:, [\"profile\", \"score\", \"query_id\", \"chain\", \"z\", \"rmsd\", \"lali\", \"nres\", \"identity\", \"description\"]]\n",
    "     .to_csv('Cas8.csv', index = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-poison",
   "metadata": {},
   "source": [
    "# Structure metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-plumbing",
   "metadata": {},
   "source": [
    "It would be highly desirable to have domains mapped onto known/predicted structures. There are a couple of strategies I can think of:\n",
    "\n",
    "1. The Pfam FTP offers a mapping of Pfam domains --> known PDB structures [here](http://ftp.ebi.ac.uk/pub/databases/Pfam/mappings/pdb_pfam_mapping.txt). There are `~75700` mappings. The drawback is this is only pfam domains and known structures.\n",
    "2. The Pfam FTP has a file of for RosettaFold  [here](http://ftp.ebi.ac.uk/pub/databases/Pfam/RoseTTAfold_aln/Pfam35.0.tar.gz). However, it is just a3m-formatted MSAs of each pfam entry.\n",
    "3. On the 'AlphaFold structures' tab of a given Pfam, there is a list of proteins \"that match this family and have AlphaFold structures\". This is nice, because it includes **predicted** structures in the AlphaFold database. However, there is no mapping of the coordinates of the pfam domain onto the predicted structure and I don't know how to download this data. I think I would first have to get a table of Pfam-->Uniprot ProteinID mappings, then get ProteinID mappings --> AlphaFold structure. To do the former, there is a [SQL](https://sparql.uniprot.org/sparql) query interface, but I don't know how to do construct the query, so I contacted the helpdesk.\n",
    "4. Entrez offers a [linkname](https://www.ncbi.nlm.nih.gov/entrez/query/static/entrezlinks.html) to go from CDD-->known PDB structure. However, I can't find the coordinates of the domain on the structure, and it only includes known PDB structures.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
