{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about setting up a SnakeMake workflow to go from SRR run Id -->--> BAM file on **BioWulf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment\n",
    "\n",
    "Login to an interactive session\n",
    ">sinteractive --cpus-per-task=6 --mem=6g\n",
    "\n",
    "Load snakemake\n",
    ">module load snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the `Snakemake` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Wrappers\n",
    "\n",
    "There are a lot of Snakemake wrappers for common tools [here](https://snakemake-wrappers.readthedocs.io/en/stable/)\n",
    "\n",
    "The top half contains the snakemake **rules** to copy-paste into the `Snakemake` file. The `wrapper` flag points to the code that it will run, which is listed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the `cluster` file\n",
    "\n",
    "For every rule, you can choose the partition/time/resources, etc. E.g.,\n",
    "```\n",
    "__default__:\n",
    "    partition: quick\n",
    "    time: 10\n",
    "    extra: \"--gres=lscratch:10\"\n",
    "hisat2:\n",
    "    partition: norm\n",
    "```\n",
    "In this example, hisat would be run on the norm partition with a walltime of 10 min and 10GB of lscratch - the latter two from the __default__ section since they are not defined for the hisat2 rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the `config` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Options for running the snakefile (from the [NIH class on using snakemake](https://github.com/NIH-HPC/snakemake-class/tree/master/exercise05)). Modify these as appropriate in the file `myprofile/config.yaml`. This refers to the `cluster` file.\n",
    " \n",
    "```\n",
    "1. -k, --keep-going: By default, snakemake will quit if a job fails (after waiting for running jobs to finish. -k will make snakemake continue with independent jobs.\n",
    "2. -w, --latency-wait, --output-wait: The amount of time snakemake will wait for output files to appear after a job has finished. This defaults to a low 5s. On the shared file systems latency may be higher. Raising it to 120s is a bit excessive, but it doesn't really hurt too much.\n",
    "3. --local-cores: The number of CPUs available for local rules\n",
    "4. --max-jobs-per-second: Max numbers of jobs to submit per second. Please be kind to the batch scheduler.\n",
    "5. --cluster: The template string used to submit each (non local) job.\n",
    "6. --jobs: The number of jobs to run concurrently.\n",
    "7. --cluster-config: The cluster config file\n",
    "```\n",
    "Example:\n",
    "\n",
    "```\n",
    "#this is config.yaml\n",
    "\n",
    "max-jobs-per-second: 1\n",
    "latency-wait: 120\n",
    "keep-going: true\n",
    "cluster: 'sbatch -c {cluster.threads} --mem={cluster.mem} --partition={cluster.partition} --time={cluster.time} {cluster.extra}'\n",
    "```\n",
    "\n",
    "2. Add the input files here, and Add the Rule parameters. T\n",
    "\n",
    "Example:\n",
    "```\n",
    "#this is config.yaml\n",
    "{ \n",
    "\t\"samples\" : {\n",
    "\t\t\"A\" : \"data/samples/A.fastq\",\n",
    "\t\t\"B\" : \"data/samples/B.fastq\"\n",
    "\t},\n",
    "\t\"samtools_view\" : {\n",
    "\t\t\"flag\" : \"0x5\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "This turns into a dictionary that can be accessed in the Snakefile code:\n",
    "```\n",
    "input:\n",
    "    lambda wildcards: config[\"samples\"][wildcards.sample]\n",
    "params:\n",
    "    flag = config[\"samtools_view\"][\"flag\"]\n",
    "shell:\n",
    "    \"samtools view -c -f {params.flag} {input} > {output}\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Snakemake rules file\n",
    "\n",
    "1. The first rule is the expected target file. It will run all rules required to generate `/counts/A.counts`\n",
    "```\n",
    "rule all:\n",
    "    input:\n",
    "        \"counts/A.counts\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run snakemake\n",
    "\n",
    "Make a script with the command line options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#! /bin/bash\n",
    "# this file is snakemake.sh\n",
    "\n",
    "module load snakemake || exit 1\n",
    "\n",
    "snakemake --profile ./myprofile --jobs 100 --cluster-config=cluster.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the master job. The arguments here (2 CPUs, 8g memory) only apply to the master job and aren't used for the subjobs (I'm pretty sure..).\n",
    ">sbatch --cpus-per-task=2 --mem=8g snakemake.sh --time=48:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job display\n",
    "#module load graphviz\n",
    "\n",
    "For the workflow:\n",
    ">snakemake --rulegraph | dot -Tpng > rulegraph.png\n",
    "\n",
    "For every job:\n",
    ">snakemake --dag | dot -Tpng > dag.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_paired_end(directory, accession, extension):\n",
    "    \"\"\"\n",
    "    Accepts a directory and tests if it contains \n",
    "    accession_1.extension AND accession_2.extension\n",
    "    \n",
    "    e.g., is_paired_end(path/to/dir, SRR001, .fasta)\n",
    "    returns TRUE if\n",
    "    path/to/dir/SRR001_1.fasta AND path/to/dir/SRR001_2.fasta\n",
    "    exist.\n",
    "    Otherwise, FALSE\n",
    "    \"\"\"\n",
    "    p = Path(directory)\n",
    "    assert p.is_dir(), f\"{directory} is not a directory\"\n",
    "    \n",
    "    paired_end1 = p.joinpath(accession + \"_1\" + extension)\n",
    "    paired_end2 = p.joinpath(accession + \"_2\" + extension)\n",
    "    \n",
    "    if paired_end1.is_file() and paired_end2.is_file():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "l1 = ['CYGL01000011.1']\n",
    "for accession in l1:\n",
    "    if is_paired_end('../../projects/human_virome_project/01annotation/contigs/', accession , '.fna') == True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
